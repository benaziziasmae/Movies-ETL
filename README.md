# Movies-ETL

## Overview of the project 

In this module, we learned how to use the Extract, Transform, Load (ETL) process to create data pipelines. A data pipeline moves data from a source to a destination, and the ETL process creates data pipelines that also transform the data along the way. Analysis is impossible without access to good data, so creating data pipelines is often the first step before any analysis can be performed. Therefore, understanding ETL is an essential skill for data analysis.

The goal of this analysis is to create an automated pipeline thatkes in new data, performs the appropriate transformations, and loads the data into existing tables. By taking code that was previously created, we refractored the code to create one function that takes in the three files—Wikipedia data, Kaggle metadata, and the MovieLens rating data—and performs the ETL process by adding the data to a PostgreSQL database.

## Resources

- Data: [wikipedia-movies](/wikipedia-movies.JSON)
- Software : PostgresSQL , Jupyter Notebook 
